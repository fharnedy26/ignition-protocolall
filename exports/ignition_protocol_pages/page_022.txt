 
Table: Runtime comparison of the Ignition engine under different workload profiles on two systems. The “Candidates” column indicates the scale of each 
run (approximate number of molecules evaluated). Times are given in seconds (and minutes in parentheses where appropriate). System A is the i7-8700 
CPU (Optiplex), System B is the i5-12th Gen CPU (personal PC). Several clear patterns emerge from these results. First, as expected, runtime grows with the 
number of candidates, and seemingly in a roughly linear fashion for these profiles. For example, a 10× increase in candidates (from ~3k in Classroom to 
~50k in Light Stress) led to about a 5× increase in time on System A (60 s to 300 s). From ~50k to ~180k (Light to Heavy) ~3.6× more candidates gave 4× 
the time (5 min to 20 min). And from ~180k to ~800k (Heavy to Brutal) ~4.4× candidates gave 3× the time (20 min to 60 min). The scaling is not perfectly 
linear across the entire range – we see some superlinear slowdowns at the high end, likely due to overheads that kick in with very large candidate sets (e.g., 
memory effects, cache misses, or increased garbage collection in Python). However, the overall trend is that more candidates directly translate to more 
time, as one would anticipate for an algorithm that processes each candidate in sequence. Second, the newer System B outperformed System A in every 
test, and the advantage became somewhat more pronounced in heavier profiles. For Ultra-snappy and Demo, the difference was small in absolute terms 
(fractions of a second) and about 25–33% in relative terms. By the time we reach Light, Heavy, and Brutal, System B was completing runs roughly 30–40% 
faster. This is likely due to a combination of factors: the i5-12th Gen has a higher per-core performance (IPC and clock speed improvements over the older 
i7-8700 architecture) and possibly more effective multi-threading or better memory bandwidth, which helps when managing large data structures. It’s 
worth noting that our code is largely single-threaded Python; however, some parts of the underlying libraries (e.g., RDKit molecule operations or maybe the 
Python interpreter’s memory management) could be taking advantage of multiple threads or at least benefiting from the CPU’s capabilities. The presence 
of additional efficiency cores on the 12th-gen i5 might also offload background tasks (like OS overhead or garbage collection) allowing the main threads to 
run more smoothly. In any case, while both systems scale similarly in terms of algorithmic complexity, System B consistently achieved better absolute 
times, which is encouraging for those wanting to run the engine on modern hardware. The performance gap suggests that optimization efforts (discussed 
below) could further leverage advanced hardware features (including the GPU, which we did not utilize yet on System B). Third, memory and overhead 
start to play a role at the largest scales. Although not explicitly shown in the table, we observed that during the Brutal run on System A (16 GB RAM), the 
system memory usage spiked to a high level, causing some minor paging and slowdown towards the end. System B (32 GB RAM) held the entire workload 
in memory more comfortably. This indicates that for extremely large runs (hundreds of thousands of molecules), memory management becomes a 
bottleneck. The algorithm spends more time allocating and deallocating objects (molecule representations, score lists, etc.), and cache efficiency drops 
when working sets grow huge. These factors likely contributed to the slightly sub-linear scaling we saw (e.g., Heavy→Brutal not scaling linearly with 
candidate count). 
Bottleneck Analysis 
Profiling the engine’s performance in these stress tests helped us pinpoint dominant bottlenecks in the current implementation: 
Python Loop Overhead: A large portion of the engine’s workflow is implemented in Python, especially the outer loops that generate mutations and apply 
filters and scoring to each candidate. Python’s interpreted nature means each iteration carries overhead. In small runs this isn’t noticeable, but in runs 
processing hundreds of thousands of molecules, the time spent simply looping and managing Python objects becomes significant. For instance, iterating 
over a list of a million candidates in pure Python can be a bottleneck compared to lower-level optimized code. 
Candidate Generation and Enumeration: The combinatorial explosion of candidates in deeper profiles means the engine has to enumerate a vast number 
of possible structures. Even though many are filtered out quickly, generating those structures (e.g., assembling fragments, assigning bonds) and 
representing them (likely as RDKit molecule objects or internal graph structures) costs CPU time and memory. In Brutal, millions of intermediate structures 
were considered. The generation process involves nested loops (over seeds, over fragments, etc.) which compound the Python overhead issue. Moreover, 
while RDKit is written in C++ and efficient at manipulating single molecules, calling it thousands of times from Python accrues overhead at the Python-C 
interface boundary. 
Scoring Calculations: The surrogate scoring for each molecule, while designed to be fast (simple property computations), still adds up when done 800k 
times. We noticed that certain property calculations – especially if they involved traversing the molecule’s structure to count functional groups or estimate 
metrics – became time-consuming at scale. These calculations are currently done for each molecule independently; there is likely repeated work (for 
example, re-calculating a fragment-based property that could be cached or reused among similar molecules). 
21 
