 
Data Structure Optimization: We noticed sorting and memory overhead issues at large scale. We could mitigate these by using more efficient data 
structures. For instance, maintaining a running bounded priority queue for top scoring molecules (instead of sorting entire huge lists) would save time – this 
way, if we only need the top 100 molecules, we don’t need to fully sort 800k entries; we can just keep the top 100 in a heap as we go. We could also 
stream processing so that we don’t keep all candidates in memory at once: e.g., generate and score candidates on the fly and immediately discard or trim 
the pool based on score thresholds. This would lower memory usage and potentially improve cache usage (processing in chunks). Some of these changes 
would require restructuring the engine’s workflow but could greatly improve scalability for extreme profiles. 
Implementing these optimizations would directly impact the engine’s capacity to integrate with more sophisticated tools in the future. In particular, an 
important future step for the Ignition engine is integrating Machine Learning (ML) predictors for more accurate scoring and property evaluation. Our current 
surrogate scoring using simple heuristics was intentionally lightweight to allow millions of evaluations. An ML model (say a neural network predicting 
ignition delay or emissions metrics) would be more expensive per evaluation, but if we improve the engine’s efficiency, we can afford to use ML on fewer, 
more promising candidates or accelerate the ML itself with hardware: 
Hierarchical Filtering with ML: The vision is to use the fast heuristic filters (as done in the current engine) to winnow down a huge chemical space to a 
manageable subset, and then apply ML predictors for final screening or reranking. The benchmarking confirms that our heuristics can process ~10^5–10^6 
candidates in under an hour on a single machine. That means we can generate a very broad pool cheaply. If we then take, say, the top 1% of those (~10^4 
molecules) and evaluate them with a more accurate ML model (which might take, for example, 0.1 seconds per molecule on CPU or thousands per second 
on a GPU), the additional cost is reasonable. But this plan only works because our engine efficiently handled the first-pass filtering. Our work aligns with 
literature suggestions that layering cheap and expensive models is a powerful approach: the cheap heuristics cast a wide net and remove the obvious bad 
candidates, enabling the expensive model to focus on a narrowed field. We saw this first-hand in our profiles – even Brutal’s million raw structures became 
800k after basic filters; if an ML model were used from the start on all million, the runtime would be prohibitive. Instead, by front-loading efficient 
pruning, we preserve the computational budget for the later stages. 
GPU-Accelerated Scoring: If we incorporate an ML model (for example, a deep neural network surrogate for fuel properties), running it on a GPU like the 
RTX 3060 Ti could dramatically speed up scoring for large batches. For instance, instead of scoring 100k molecules one-by-one on CPU, we could batch 
them in forward passes of a neural network on GPU, evaluating thousands in parallel. Our benchmarking suggests that the rest of the engine (generation, 
filtering) can keep up with this if optimized; the current bottlenecks would shift. We might find that with a GPU in play, the molecule generation step 
(fragment assembly and conversion to model input) becomes the slowest part, in which case we’d optimize that with multi-threading or C++ 
implementation. 
Real-time Feedback and Iterative ML Guidance: A faster engine opens the door to more interactive or iterative use of ML. For example, we could run a 
genetic algorithm or reinforcement learning loop where an ML model guides molecule generation in real time. This requires the engine to quickly propose 
candidates, get them evaluated by ML, and iterate. If each loop takes too long, the approach is not practical. But our stress tests show that with the current 
setup, even generating 50k candidates can be done in a few minutes. With further efficiency improvements, we could potentially generate and evaluate 
hundreds of candidates per second. That kind of speed means an ML-guided search agent could feasibly steer the engine in an online manner, evaluating 
many possibilities without waiting hours for each cycle. 
In conclusion, treating the Ignition engine as a computational benchmarking tool has been enlightening. We quantified its performance across a spectrum 
of loads and identified where the implementation strains under pressure. The exercise reinforced the value of our design choices (like using heuristic filters 
up front) and highlighted the need for code optimizations (loop vectorization, parallelism, better data handling) as we move toward integrating heavier ML 
components. By improving efficiency now, we set the stage for a more powerful integrated platform later – one that can explore chemical space broadly 
with heuristics and deeply with ML, all within feasible time. This benchmarking forms the foundation for those next steps, ensuring that our “Ignition” 
engine can rev at the required speed when advanced predictive models are brought into the loop. 
 
23 
